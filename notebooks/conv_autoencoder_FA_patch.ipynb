{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "\n",
    "# Custom Dataset Loader for Unlabeled TIFF Images\n",
    "class UnlabeledTIFFDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.lower().endswith(('tif', 'tiff'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = tiff.imread(img_path).astype(np.float32)   \n",
    "            image = image * 240\n",
    "            image[image > 254] = 254\n",
    "            if image.ndim == 3:\n",
    "                image = image[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping unreadable image {img_path} - {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.image_paths))\n",
    "\n",
    "        image = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "        image = image.squeeze(1)\n",
    "        return image, 0\n",
    "\n",
    "# Step 1: Define a standard Autoencoder (AE)\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, latent_dim=8):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_latent = nn.Linear(128 * 8 * 8, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 128 * 8 * 8)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_latent(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z).view(-1, 128, 8, 8)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "# Step 2: Train Autoencoder and Extract Latent Representations\n",
    "def train_ae(model, train_loader, val_loader, epochs=50, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            recon, _ = model(x)\n",
    "            loss = loss_fn(recon, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, _ in val_loader:\n",
    "                x = x.to(device)\n",
    "                recon, _ = model(x)\n",
    "                loss = loss_fn(recon, x)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load Data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = '/mnt/d/lding/FA/analysis_results/panel_annabel_20250217_correction/test/code_org_202503013_seg/patches64_50p_B'\n",
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = UnlabeledTIFFDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Train AE\n",
    "ae = AE().to(device)\n",
    "ae = train_ae(ae, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Step 3: Cluster in Latent Space\n",
    "\n",
    "def cluster_latents(model, dataloader, num_clusters=4):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            _, z = model(x)\n",
    "            latents.append(z.cpu().numpy())\n",
    "            images.append(x.cpu())\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    images = torch.cat(images, dim=0)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(latents)\n",
    "    labels = kmeans.labels_\n",
    "    return latents, labels, images\n",
    "\n",
    "# Step 4: Visualize Clusters with t-SNE and Reconstructed Images\n",
    "\n",
    "def visualize_clusters(latents, labels, images, model):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(latents_2d[:, 0], latents_2d[:, 1], c=labels, cmap='tab10', alpha=0.7, s=0.5)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.title(\"2D Visualization of Latent Space using t-SNE\")\n",
    "    plt.show()\n",
    "\n",
    "    selected_images = []\n",
    "    selected_labels = set()\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in selected_labels:\n",
    "            selected_labels.add(label)\n",
    "            selected_images.append(images[i])\n",
    "        if len(selected_labels) == 4:\n",
    "            break\n",
    "\n",
    "    selected_images = torch.stack(selected_images).to(device)\n",
    "    reconstructed, _ = model(selected_images)\n",
    "\n",
    "    fig, axes = plt.subplots(2, len(selected_images), figsize=(15, 5))\n",
    "    for i in range(len(selected_images)):\n",
    "        axes[0, i].imshow(selected_images[i].squeeze().cpu(), cmap='gray', vmax=1.0, vmin=0)\n",
    "        axes[1, i].imshow(reconstructed[i].squeeze().cpu().detach(), cmap='gray', vmax=1.0, vmin=0)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Step 5: Show multiple decoded images per cluster\n",
    "\n",
    "def cluster_and_display_images(latents, labels, images, model, num_samples_per_cluster=10):\n",
    "    unique_labels = np.unique(labels)\n",
    "    fig, axes = plt.subplots(len(unique_labels)*2, num_samples_per_cluster, figsize=(num_samples_per_cluster*1.5, len(unique_labels)*2))\n",
    "\n",
    "    for cluster_idx, cluster_label in enumerate(unique_labels):\n",
    "        cluster_indices = np.where(labels == cluster_label)[0][:num_samples_per_cluster]\n",
    "        cluster_images = images[cluster_indices].to(device)\n",
    "        recon_images, _ = model(cluster_images)\n",
    "\n",
    "        for i in range(num_samples_per_cluster):\n",
    "            axes[2 * cluster_idx, i].imshow(cluster_images[i].squeeze().cpu(), cmap='gray', vmin=0, vmax=1.0)\n",
    "            axes[2 * cluster_idx + 1, i].imshow(recon_images[i].squeeze().cpu(), cmap='gray', vmin=0, vmax=1.0)\n",
    "            axes[2 * cluster_idx, i].axis('off')\n",
    "            axes[2 * cluster_idx + 1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, labels, images = cluster_latents(ae, train_loader, num_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(latents, labels, images, ae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_display_images(latents, labels, images, ae, num_samples_per_cluster=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
